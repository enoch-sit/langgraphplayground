# ğŸš€ LangGraph Playground - Complete Guide

> **Interactive playground for LangGraph concepts with AWS Bedrock Nova Lite and Human-in-the-Loop**

---

## ğŸ“‹ Table of Contents

1. [Quick Start](#-quick-start)
2. [What This Is](#-what-this-is)
3. [Architecture](#-architecture)
4. [Setup & Installation](#-setup--installation)
5. [Deployment](#-deployment)
6. [How It Works](#-how-it-works)
7. [API Reference](#-api-reference)
8. [Troubleshooting](#-troubleshooting)
9. [Customization](#-customization)

---

## âš¡ Quick Start

### Windows (Simplest)

```bash
cd c:\Users\user\Documents\langgraphplayground
setup.bat
```

Open: http://localhost:2024

### Docker

```bash
docker-compose up -d
```

Open: http://localhost:2024

### Manual

```bash
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your credentials
uvicorn src.agent.webapp:app --host 0.0.0.0 --port 2024 --reload
```

---

## ğŸ¯ What This Is

A **production-ready LangGraph playground** that demonstrates:

- âœ… **LangGraph Agents** with AWS Bedrock Nova Lite
- âœ… **Human-in-the-Loop (HITL)** approval workflow
- âœ… **NLP-based Tool Detection** (workaround for models without native tool calling)
- âœ… **State Management** with checkpoints and persistence
- âœ… **Interactive Web UI** with thread management
- âœ… **FastAPI Backend** serving both API and UI on single port
- âœ… **Streaming Support** for real-time responses

### Why This Exists

AWS Bedrock Nova Lite doesn't support native tool calling (structured outputs). This playground implements an **NLP workaround** using system prompts and JSON parsing to detect tool calls, making it work seamlessly with LangGraph's HITL features.

---

## ğŸ—ï¸ Architecture

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Browser (http://localhost:2024)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ All HTTP/WebSocket requests
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      FastAPI Server (Port 2024)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Routes:                                    â”‚ â”‚
â”‚  â”‚  GET  /           â†’ UI (index.html)       â”‚ â”‚
â”‚  â”‚  GET  /docs       â†’ API Documentation     â”‚ â”‚
â”‚  â”‚  POST /threads    â†’ Thread Management     â”‚ â”‚
â”‚  â”‚  POST /runs/*     â†’ Agent Execution       â”‚ â”‚
â”‚  â”‚  GET  /graph/info â†’ Graph Structure       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ LangGraph Agent:                           â”‚ â”‚
â”‚  â”‚  â€¢ Call Model (AWS Bedrock Nova Lite)     â”‚ â”‚
â”‚  â”‚  â€¢ Parse Tool Calls (NLP Detection)       â”‚ â”‚
â”‚  â”‚  â€¢ Interrupt for HITL                     â”‚ â”‚
â”‚  â”‚  â€¢ Execute Tools                          â”‚ â”‚
â”‚  â”‚  â€¢ State Persistence (MemorySaver)        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚
         â–¼                      â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ AWS Bedrock â”‚      â”‚  Tavily Search  â”‚
  â”‚  Nova Lite  â”‚      â”‚   (Web Search)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Components

**Backend (`src/agent/`):**
- `webapp.py` - FastAPI application with 10+ endpoints
- `graph.py` - LangGraph agent with NLP tool detection
- `tools.py` - Tool definitions (search, calculator, travel budget)

**Frontend (`src/ui/`):**
- `index.html` - Single-page vanilla JS application

**Configuration:**
- `langgraph.json` - LangGraph configuration
- `docker-compose.yml` - Container orchestration
- `.env` - Environment variables (AWS, Tavily credentials)

### Single-Port Design

**Everything runs on port 2024:**
- âœ… Web UI at `/`
- âœ… API endpoints at `/threads`, `/runs`, etc.
- âœ… API documentation at `/docs`
- âœ… No CORS issues (same-origin requests)
- âœ… Simple deployment

---

## ğŸ”§ Setup & Installation

### Prerequisites

- Python 3.11+
- AWS Bedrock access (Nova Lite model)
- Tavily API key (for search tool)
- Docker (optional, for containerized deployment)

### Step 1: Clone & Install

```bash
cd c:\Users\user\Documents\langgraphplayground

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Configure Environment

```bash
# Copy example to create your .env
cp .env.example .env

# Edit .env with your credentials
notepad .env
```

**Required variables:**

```env
# AWS Bedrock Configuration
AWS_ACCESS_KEY_ID=your_access_key_here
AWS_SECRET_ACCESS_KEY=your_secret_key_here
AWS_REGION=us-east-1

# Tavily API Key (for search tool)
TAVILY_API_KEY=your_tavily_api_key_here

# Model Configuration
AWS_BEDROCK_MODEL=amazon.nova-lite-v1:0
MODEL_TEMPERATURE=0.3
MODEL_MAX_TOKENS=4096
```

### Step 3: Run

**Option A: Direct (Development)**
```bash
uvicorn src.agent.webapp:app --host 0.0.0.0 --port 2024 --reload
```

**Option B: LangGraph CLI**
```bash
langgraph dev --port 2024
```

**Option C: Docker**
```bash
docker-compose up -d
```

**Option D: Docker with Fresh Build**
```bash
chmod +x dockerSetup.sh
./dockerSetup.sh
```

### Step 4: Verify

Open http://localhost:2024 in your browser.

You should see the LangGraph Playground interface!

---

## ğŸš€ Deployment

### Production with Nginx

#### Your Server Setup

If you have Flowise or other apps running, you can add LangGraph alongside them:

```nginx
# Add this to /etc/nginx/nginx.conf in the http block
map $http_upgrade $connection_upgrade {
    default upgrade;
    '' close;
}

# Add this to /etc/nginx/sites-available/your-site
server {
    listen 443 ssl;
    server_name your-domain.com;
    
    # ... your SSL config ...
    
    # LangGraph Playground
    location /langgraphplayground/ {
        rewrite ^/langgraphplayground/(.*) /$1 break;
        proxy_pass http://localhost:2024/;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection $connection_upgrade;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_buffering off;
        proxy_cache off;
        proxy_read_timeout 300s;
        proxy_connect_timeout 75s;
    }
    
    # Your other apps (e.g., Flowise)
    location / {
        proxy_pass http://localhost:3000;
        # ... other config ...
    }
}
```

#### Apply Configuration

```bash
# Test nginx config
sudo nginx -t

# Reload nginx
sudo systemctl reload nginx
```

#### Access

- Production: `https://your-domain.com/langgraphplayground`
- Local: `http://localhost:2024`

### Docker Production Deployment

```bash
# Build and start
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

### Port Isolation

The playground uses **port 2024** exclusively. This ensures:
- âœ… No conflicts with other services (e.g., Flowise on 3000)
- âœ… Easy to proxy via nginx
- âœ… Can run alongside any existing applications

---

## ğŸ’¡ How It Works

### NLP-Based Tool Detection

Since AWS Nova Lite doesn't support native tool calling, we use an NLP workaround:

#### 1. System Prompt

The LLM is instructed to output tool calls as JSON:

```python
SYSTEM_PROMPT = """You are a helpful assistant with access to tools.
When you need to use a tool, respond with JSON:
{"tool": "tool_name", "args": {"param": "value"}}

Available tools:
- tavily_search_results_json: Search the web
- get_travel_budget: Calculate travel costs
- calculator: Evaluate math expressions
"""
```

#### 2. Response Parsing

The `parse_tool_call()` function detects tool calls:

```python
def parse_tool_call(content: str) -> Optional[Dict[str, Any]]:
    # Try JSON parsing first
    try:
        parsed = json.loads(content)
        if "tool" in parsed and "args" in parsed:
            return parsed
    except:
        pass
    
    # Fallback to regex patterns
    patterns = [
        r'\{"tool":\s*"([^"]+)",\s*"args":\s*(\{[^}]+\})\}',
        # ... more patterns ...
    ]
    # ... pattern matching logic ...
```

**Accuracy:** 80-90% with proper prompt tuning and temperature 0.3

#### 3. Mock ToolCall Creation

When detected, we create a mock `tool_calls` attribute:

```python
if tool_call:
    message.tool_calls = [
        {
            "name": tool_call["tool"],
            "args": tool_call["args"],
            "id": str(uuid.uuid4())
        }
    ]
```

This makes it compatible with LangGraph's standard tool execution flow!

### Human-in-the-Loop (HITL)

#### Graph Configuration

```python
graph = graph_builder.compile(
    checkpointer=memory,
    interrupt_before=["tools"]  # â† Stops before tool execution
)
```

#### Workflow

```
1. User sends message
   â†“
2. LLM generates response (with tool call)
   â†“
3. Graph INTERRUPTS before tools node
   â†“
4. UI shows approval dialog
   â†“
5. User approves/rejects
   â†“
6. POST /runs/resume
   â†“
7. Tools execute (if approved)
   â†“
8. Continue conversation
```

#### API Flow

```javascript
// 1. Initial run
POST /runs/invoke
{
    "thread_id": "abc-123",
    "message": "Search for Python tutorials",
    "use_hitl": true
}

// Response (interrupted)
{
    "status": "interrupted",
    "next_step": "tools",
    "pending_tool_calls": [
        {
            "name": "tavily_search_results_json",
            "args": {"query": "Python tutorials"}
        }
    ]
}

// 2. User approves
POST /runs/resume
{
    "thread_id": "abc-123",
    "approved": true
}

// Response (completed)
{
    "status": "completed",
    "messages": [...]
}
```

### State Management

**MemorySaver Checkpointer:**
- Stores conversation state in memory
- Each thread has isolated state
- Supports time-travel (rewind to checkpoints)
- Automatic persistence across requests

**State Structure:**

```python
class AgentState(TypedDict):
    messages: Annotated[list, add_messages]
```

Messages include:
- `HumanMessage` - User inputs
- `AIMessage` - LLM responses (with tool_calls)
- `ToolMessage` - Tool execution results

---

## ğŸ“š API Reference

### Base URL

- Local: `http://localhost:2024`
- Production: `https://your-domain.com/langgraphplayground`

### Endpoints

#### `POST /threads`

Create a new conversation thread.

**Request:**
```json
{
    "thread_id": "optional-custom-id"
}
```

**Response:**
```json
{
    "thread_id": "abc-123",
    "created": true
}
```

#### `GET /threads/{thread_id}`

Get thread details.

**Response:**
```json
{
    "thread_id": "abc-123",
    "exists": true
}
```

#### `GET /threads/{thread_id}/state`

Get current conversation state.

**Response:**
```json
{
    "messages": [...],
    "next": ["tools"],
    "checkpoint_id": "checkpoint-uuid"
}
```

#### `POST /runs/invoke`

Run the agent with a message.

**Request:**
```json
{
    "thread_id": "abc-123",
    "message": "Search for Python tutorials",
    "use_hitl": true
}
```

**Response (Interrupted):**
```json
{
    "status": "interrupted",
    "next_step": "tools",
    "pending_tool_calls": [
        {
            "name": "tavily_search_results_json",
            "args": {"query": "Python tutorials"},
            "id": "call-123"
        }
    ]
}
```

**Response (Completed):**
```json
{
    "status": "completed",
    "messages": [
        {"type": "human", "content": "..."},
        {"type": "ai", "content": "..."}
    ]
}
```

#### `POST /runs/resume`

Resume execution after HITL approval.

**Request:**
```json
{
    "thread_id": "abc-123",
    "approved": true,
    "modified_args": {
        "query": "Modified search query"
    }
}
```

**Response:**
```json
{
    "status": "completed",
    "messages": [...]
}
```

#### `GET /threads/{thread_id}/history`

Get checkpoint history.

**Response:**
```json
{
    "checkpoints": [
        {
            "checkpoint_id": "uuid-1",
            "parent_id": null,
            "created_at": "2024-10-03T10:00:00Z"
        }
    ]
}
```

#### `GET /graph/info`

Get graph structure information.

**Response:**
```json
{
    "nodes": ["__start__", "agent", "tools", "__end__"],
    "edges": [...],
    "interrupt_before": ["tools"],
    "checkpointer": "MemorySaver"
}
```

#### `GET /docs`

Interactive API documentation (Swagger UI).

#### `GET /health`

Health check endpoint.

**Response:**
```json
{
    "status": "healthy",
    "service": "langgraph-playground"
}
```

---

## ğŸ”§ Troubleshooting

### Common Issues

#### Port Already in Use

```bash
# Check what's using port 2024
netstat -ano | findstr :2024

# Kill the process (Windows)
taskkill /PID <PID> /F
```

#### AWS Credentials Error

```
Error: Unable to locate credentials
```

**Fix:** Verify `.env` file has correct AWS credentials:
```env
AWS_ACCESS_KEY_ID=AKIA...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1
```

#### Tavily API Error

```
Error: 401 Unauthorized
```

**Fix:** Get API key from https://tavily.com and add to `.env`:
```env
TAVILY_API_KEY=tvly-...
```

#### Tool Detection Not Working

**Symptoms:** LLM doesn't trigger tools, or responses are text instead of JSON.

**Fixes:**
1. Lower temperature (0.3 or below) in `.env`
2. Check system prompt includes tool instructions
3. Verify model is `amazon.nova-lite-v1:0`

#### Nginx 502 Bad Gateway

**Cause:** FastAPI not running on port 2024.

**Fix:**
```bash
# Check if running
curl http://localhost:2024/health

# Start if not running
cd /path/to/langgraphplayground
docker-compose up -d
# or
uvicorn src.agent.webapp:app --host 0.0.0.0 --port 2024
```

#### Frontend Can't Reach API (with nginx subpath)

**Cause:** JavaScript `API_BASE` not detecting subpath.

**Fix:** Already patched in `src/ui/index.html` with `getApiBase()` function. If issue persists, check browser console:
```javascript
console.log('API_BASE set to:', API_BASE);
```

Should show: `https://your-domain.com/langgraphplayground` (not just origin).

### Debug Mode

Enable verbose logging:

```python
# Add to src/agent/webapp.py
import logging
logging.basicConfig(level=logging.DEBUG)
```

View logs:
```bash
# Docker
docker-compose logs -f

# Direct run
# Logs appear in terminal
```

---

## ğŸ¨ Customization

### Add Custom Tools

**Edit `src/agent/tools.py`:**

```python
from langchain_core.tools import tool

@tool
def my_custom_tool(param: str) -> str:
    """Description of what this tool does."""
    # Your logic here
    return f"Result for {param}"

# Add to tools list
tools = [
    search_tool,
    get_travel_budget,
    calculator,
    my_custom_tool  # â† Add here
]
```

**Update system prompt in `src/agent/graph.py`:**

```python
SYSTEM_PROMPT = """...
Available tools:
- my_custom_tool: Description
...
"""
```

### Modify UI Styling

**Edit `src/ui/index.html`:**

```html
<style>
    /* Change gradient background */
    body {
        background: linear-gradient(135deg, #your-color1 0%, #your-color2 100%);
    }
    
    /* Change card colors */
    .panel {
        background: rgba(255, 255, 255, 0.95);
        /* Your custom styles */
    }
</style>
```

### Change Model

**Edit `.env`:**

```env
# Use different Bedrock model
AWS_BEDROCK_MODEL=anthropic.claude-3-sonnet-20240229-v1:0

# Or any other model
AWS_BEDROCK_MODEL=amazon.titan-text-express-v1
```

**Note:** If using a model with native tool calling, you can simplify `graph.py` by removing the NLP parsing logic.

### Add Authentication

**Edit `src/agent/webapp.py`:**

```python
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer

security = HTTPBearer()

async def verify_token(credentials = Depends(security)):
    if credentials.credentials != "your-secret-token":
        raise HTTPException(status_code=401, detail="Invalid token")
    return credentials

# Protect routes
@app.post("/threads", dependencies=[Depends(verify_token)])
async def create_thread():
    # ...
```

### Environment Variables

All configurable options in `.env`:

```env
# AWS Configuration
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1

# Model Settings
AWS_BEDROCK_MODEL=amazon.nova-lite-v1:0
MODEL_TEMPERATURE=0.3
MODEL_MAX_TOKENS=4096

# API Keys
TAVILY_API_KEY=

# Server Settings
PORT=2024
HOST=0.0.0.0
```

---

## ğŸ“‚ Project Structure

```
langgraphplayground/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ graph.py          # LangGraph agent (NLP tool detection)
â”‚   â”‚   â”œâ”€â”€ tools.py          # Tool definitions
â”‚   â”‚   â””â”€â”€ webapp.py         # FastAPI application
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ index.html        # Web interface
â”œâ”€â”€ .env                      # Environment variables (create from .env.example)
â”œâ”€â”€ .env.example              # Example environment configuration
â”œâ”€â”€ .gitignore                # Git ignore patterns
â”œâ”€â”€ docker-compose.yml        # Docker Compose configuration
â”œâ”€â”€ Dockerfile                # Container image definition
â”œâ”€â”€ dockerSetup.sh            # Docker setup script (Unix/Linux)
â”œâ”€â”€ GUIDE.md                  # This comprehensive guide
â”œâ”€â”€ langgraph.json            # LangGraph configuration
â”œâ”€â”€ nginx.conf.example        # Example nginx configuration
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.bat                 # Windows setup script
â””â”€â”€ setup.sh                  # Unix/Linux setup script
```

---

## ğŸ“ Key Concepts

### LangGraph Nodes

- **agent** - Calls LLM, parses responses, detects tool calls
- **tools** - Executes tools (search, calculator, etc.)

### Edges

- **Conditional** - Routes based on whether tools are needed
- **Normal** - Direct connections between nodes

### Checkpointer

- **MemorySaver** - In-memory state persistence
- Enables HITL interrupts and state rewind
- Thread-isolated storage

### Tool Detection (NLP)

- System prompt instructs JSON format
- Parser looks for `{"tool": "...", "args": {...}}`
- Falls back to regex if JSON fails
- 80-90% accuracy with tuning

---

## ğŸš€ Next Steps

1. **Test Locally** - Run `setup.bat` and explore at http://localhost:2024
2. **Deploy to Server** - Use Docker and nginx configuration
3. **Add Custom Tools** - Extend functionality for your use case
4. **Integrate with Apps** - Use API endpoints in your applications
5. **Monitor & Scale** - Add logging, metrics, and horizontal scaling

---

## ğŸ“ Support

- **Issues:** Check [Troubleshooting](#-troubleshooting) section
- **API Docs:** http://localhost:2024/docs
- **LangGraph Docs:** https://langchain-ai.github.io/langgraph/

---

## ğŸ‰ Features Summary

âœ… **Complete LangGraph Implementation** with state management  
âœ… **Human-in-the-Loop** approval workflow  
âœ… **NLP Tool Detection** for models without native tool calling  
âœ… **Single-Port Architecture** (FastAPI serves everything)  
âœ… **Interactive Web UI** with thread management  
âœ… **Real-time Streaming** support  
âœ… **Production-Ready** with Docker and nginx configs  
âœ… **Extensible** - easy to add custom tools and styling  
âœ… **Well-Documented** - comprehensive API reference  
âœ… **AWS Bedrock Integration** with Nova Lite model  

---

**Built with:** Python, FastAPI, LangGraph, AWS Bedrock, Tavily API, Vanilla JavaScript

**License:** Open source - use freely for your projects!

---

*Happy exploring with LangGraph! ğŸ¯*
